{"cells":[{"cell_type":"markdown","metadata":{"id":"pSy-sfxOsclS"},"source":["# CS 447 Homework 2 $-$ Word Embeddings \\& Text Classification with Neural Networks\n","In this homework, you will first train word embeddings using the continuous-bag-of-words (CBOW) method. Then, you will build a convolutional neural network (CNN) classifier to detect the sentiment of movie reviews using the IMDb movie reviews dataset.\n","\n","In addition to the Pytorch tutorial we have provided online, we highly recommend that you take a look at the PyTorch tutorials before starting this assignment:\n","<ul>\n","<li><a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">https://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a>\n","<li><a href=\"https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\">https://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a>\n","<li><a href=\"https://github.com/yunjey/pytorch-tutorial\">https://github.com/yunjey/pytorch-tutorial</a>\n","</ul>\n","\n","<font color='green'><b>Hint:</b> We suggest that you work on this homework in <b>CPU</b> until you are ready to train. At that point, you should switch your runtime to <b>GPU</b>. You can do this by going to <TT>Runtime > Change Runtime Type</TT> and select \"GPU\" from the dropdown menu.\n","* You will find it easier to debug on CPU, and the error messages will be more understandable.\n","* Google monitors your GPU usage and will occasionally restrict GPU access if you use it too much. In these cases, you can either switch to a different Google account or wait for your access to be restored.</font>\n","\n","We have imported all the libraries you need to do this homework. <b>You should not import any extra libraries. Furthermore, you should not write any code outside of TODO sections.</b> If you do, the autograder will fail to run your code.\n","\n","**Reminder: The course policy of this class prohibits the use of AI tools to help with coding or debugging, such as Chat-GPT or Colab's in-built Gemini assistant. Further, you may not look at or copy from code repositories online; and while you may discuss the homework with your classmates, you may *not* share code with each other.** You are of course welcome to look at general Python materials (such as Python or Pytorch tutorials and documentation)."]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"xYoUsZjO-ZYZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 1: Continuous-Bag-of-Words (CBOW) Embeddings [50 points]\n","\n","In the first part of this assignment you will learn dense word embeddings based on the word2vec paradigm. In particular, you will use the continuous-bag-of-words approach, which trains a model to predict a word based on the embeddings of surrounding words. For example, in the sentence \"the man walks the dog in the park\", the embeddings for the words (\"man, \"walks\", \"dog\", \"in\") will be used to predict the word \"the\" (if your context size is 2 on each side of the target word)."],"metadata":{"id":"lCBhL7pR9E3N"}},{"cell_type":"markdown","source":["## Download \\& Preprocess the Data\n","First we will download the dataset."],"metadata":{"id":"4hw9Zahp38Iv"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","if __name__=='__main__':\n","    print('Using device:', DEVICE)"],"metadata":{"id":"Q8Ak_A674YAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we download the data. As with Homework 1, we will use WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2.preprocess\n","\n","After downloading the data, we preprocess the text as in Homework 1. <i>You do not need to edit this code.</i>\n","\n","* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;In this homework, we are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n","\n","* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow your models to generate sentences that have realistic beginnings and endings.\n","\n","* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`). The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead.\n","\n","We provide you with preprocessing code here, and you should not modify it."],"metadata":{"id":"nLbs3wht5weP"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","# Constants (feel free to use these in your code, but do not change them)\n","CBOW_START = \"<s>\"   # Start-of-sentence token\n","CBOW_END = \"</s>\"    # End-of-sentence-token\n","CBOW_UNK = \"<UNK>\"   # Unknown word token"],"metadata":{"id":"Lbkr6F2N9lvd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","import os\n","import random\n","import sys\n","from urllib.request import urlretrieve\n","\n","def cbow_preprocess(data, vocab=None, do_lowercase=True):\n","    final_data = []\n","    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n","    for paragraph in data:\n","        paragraph = [x if x != '<unk>' else CBOW_UNK for x in paragraph.split()]\n","        if vocab is not None:\n","            paragraph = [x if x in vocab else CBOW_UNK for x in paragraph]\n","        if paragraph == [] or paragraph.count('=') >= 2: continue\n","        sen = []\n","        prev_punct, prev_quot = False, False\n","        for word in paragraph:\n","            if prev_quot:\n","                if word[0] not in lowercase:\n","                    final_data.append(sen)\n","                    sen = []\n","                    prev_punct, prev_quot = False, False\n","            if prev_punct:\n","                if word == '\"':\n","                    prev_punct, prev_quot = False, True\n","                else:\n","                    if word[0] not in lowercase:\n","                        final_data.append(sen)\n","                        sen = []\n","                        prev_punct, prev_quot = False, False\n","            if word in {'.', '?', '!'}: prev_punct = True\n","            sen += [word]\n","        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n","        final_data.append(sen)\n","    vocab_was_none = vocab is None\n","    if vocab is None:\n","        vocab = {}\n","    for i in range(len(final_data)):\n","        # Make words lowercase for this assignment\n","        final_data[i] = [x.lower() if do_lowercase and x != CBOW_UNK else x for x in final_data[i]]\n","        final_data[i] = [CBOW_START] + final_data[i] + [CBOW_END]\n","        if vocab_was_none:\n","            for word in final_data[i]:\n","                vocab[word] = vocab.get(word, 0) + 1\n","    return final_data, vocab\n","\n","def getDataset():\n","    path = './train.txt'\n","    url = 'https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/train.txt'\n","    if os.path.exists(path):\n","        print(\"train dataset already downloaded\")\n","    else:\n","        urlretrieve(url, path)\n","    dataset = open(path).read().split('\\n')\n","    train_dataset, vocab = cbow_preprocess(dataset)\n","    return train_dataset, vocab\n","\n","if __name__=='__main__':\n","    sentences, vocab = getDataset()"],"metadata":{"id":"rnSyKLij9ocH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the next cell to see 10 random sentences of the data."],"metadata":{"id":"sRDXzF956c6n"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    for x in random.sample(sentences, 10):\n","        print (x)"],"metadata":{"id":"HLSERYow7Cbt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##<font color='red'>TODO:</font> Define the Dataset Class [15 points]\n","In the following cell, we will define the <b>dataset</b> class. The dataset contains input-output pairs for each training example we will provide to the model. You need to implement the following functions:\n","\n","*   <b>` make_training_examples(self)`:</b>  <b>[5 points]</b> Each training example will be a list of <em>context</em> words along with a <em>target</em> word. The context words consist of $c$ words on either side of the target word; hence, each list of context words has size $2c$. The goal will be to have your model predict the target word from the context words. Thus, you must convert each sentence into a series of context-target pairs, as follows:\n","<ul>\n","<li>For each sentence $s=[w_1,w_2,...,w_n]$ and a context size $c$, compute the following (context, target) pairs:<br>&emsp;&emsp;&emsp;&emsp;$([w_1,...,w_c,w_{c+2},...,w_{2c+1}]$, $w_{c+1}$)<br>&emsp;&emsp;&emsp;&emsp;$([w_2,...,w_{c+1},w_{c+3},...,w_{2c+2}]$, $w_{c+2}$)<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\vdots$<br>&emsp;&emsp;&emsp;&emsp;$([w_{n-2c},...,w_{n-c-1},w_{n-c+1},...,w_{n}]$, $w_{n-c}$)<br>For example, suppose your sentence is \"the man walks the dog in the park\" and the context size is $c=2$. Your method should find the following training pairs:<br>&emsp;&emsp;&emsp;&emsp;([\"the\", \"man\", \"the\", \"dog\"], \"walks\")<br>&emsp;&emsp;&emsp;&emsp;([\"man\", \"walks\", \"dog\", \"in\"], \"the\")<br>&emsp;&emsp;&emsp;&emsp;([\"walks\", \"the\", \"in\", \"the\"], \"dog\")<br>&emsp;&emsp;&emsp;&emsp;([\"the\", \"dog\", \"the\", \"park\"], \"in\")<br>Of course, the sentences in your dataset have start- and end-of-sentence tokens as well, which you should treat as any other word.\n","</ul>\n","This function should return a list of <b>all</b> such training pairs.\n","\n","*   <b>` build_dictionaries(self, vocab)`:</b>  <b>[4 points]</b> Creates the dictionaries `word2idx` and `idx2word`. You will represent each word in the vocabulary with a unique index, and keep track of this in these dictionaries. The input `vocab` is a list of words: you must assign indexes in the order the words appear in this list.\n","\n","* <b>`get_context_vector(self, idx)`:</b> Returns a vector representing the <em>context</em> of the `idx`th training example. Specifically, if the context size is $c$, this should be a tensor of $2c$ word indices corresponding to the context words of the `idx`th example.\n","\n","   <font color='green'><b>Hint:</b> You may want to pre-compute and save all context vectors (using word indices rather than the words themselves) in `__init__(...)`, and then access these in `get_context_vector(self, idx)`. This would give you a slight speedup at train time.</font>\n","\n","*   <b>`get_target_index(self, idx) `</b>: Return the target word index for the `idx`th training example.\n","\n","*  <b> ` __len__(self) `: [1 points]</b> Return the total number of training examples in the dataset as an `int`.\n","\n","*   <b>` __getitem__(self, idx)`:</b> <b>[5 points]</b> Return the `idx`th training example as a tuple of `(context_vector, target_word_index)`. You should use the ` get_context_vector(self, idx) ` and ` get_label(self, idx) ` functions here."],"metadata":{"id":"aZwVLt4n7ksc"}},{"cell_type":"code","source":["from torch.utils import data\n","from collections import defaultdict\n","\n","class CbowDataset(data.Dataset):\n","    def __init__(self, sentences, vocab, context_size):\n","        ##### DO NOT EDIT #####\n","\n","        assert CBOW_START in vocab and CBOW_END in vocab and CBOW_UNK in vocab\n","        self.sentences = sentences\n","        self.context_size = context_size\n","\n","        self.training_examples = []\n","        self.make_training_examples()\n","\n","        self.word2idx = {} # Mapping of word to index\n","        self.idx2word = {} # Mapping of index to word\n","        self.build_dictionaries(sorted(vocab.keys()))\n","        self.vocab_size = len(self.word2idx)\n","\n","    def make_training_examples(self):\n","        '''\n","        Builds a list of context-target_word pairs that will be used as training examples for the model and stores them in\n","        self.training_examples.\n","        Each example is a (context, target_word) tuple, where context is a list of strings of size 2*context_size and\n","        target_word is simply a string.\n","        Returns nothing.\n","        '''\n","\n","        ##### TODO #####\n","        # For each sentence, loop over each word in the sentence. If there are c words before and c words after the word,\n","        # make a (context, word) pair, where context is a list made up of the c words before the word and the c words\n","        # after the word (in the same order they appear in the sentence). Append this (context, word) pair to self.training_examples.\n","\n","        pass\n","\n","    def build_dictionaries(self, vocab):\n","        '''\n","        Builds the dictionaries self.idx2word and self.word2idx. Make sure that you assign indices\n","        in the order the words appear in vocab (a list of words).\n","        Returns nothing.\n","        '''\n","\n","        ##### TODO #####\n","\n","        pass\n","\n","    def get_context_vector(self, idx):\n","        '''\n","        Returns the context vector (as a torch.tensor) for the training example at index idx.\n","        This is is a tensor containing the indices of each word in the context.\n","        '''\n","        assert len(self.training_examples) > 0\n","\n","        ##### TODO #####\n","\n","        return None\n","\n","    def get_target_index(self, idx):\n","        '''\n","        Returns the index of the target word (as type int) of the training example at index idx.\n","        '''\n","        ##### TODO #####\n","\n","        return None\n","\n","    def __len__(self):\n","        '''\n","        Returns the number of training examples (as type int) in the dataset\n","        '''\n","        ##### TODO #####\n","\n","        return None\n","\n","    def __getitem__(self, idx):\n","        '''\n","        Returns the context vector (as a torch.tensor) and target index (as type int) of the training example at index idx.\n","        '''\n","        ##### TODO #####\n","\n","        return None, None"],"metadata":{"id":"beABjh82XyXA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sanity Check: Dataset Class\n","\n","The code below runs a sanity check for your `CbowDataset` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug.\n","\n","You do <b>not</b> need to edit this cell."],"metadata":{"id":"hqmwBVMDkqEQ"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","def sanityCheckCbowDataset():\n","    #\tRead in the sample corpus\n","    test_sents = [['<s>', 'the', 'man', 'walks', 'the', 'dog', 'in', 'the', 'park', '</s>'],\n","            ['<s>', 'i', 'saw', 'the', 'man', 'with', 'the', 'telescope', 'on', 'the', CBOW_UNK, '</s>']]\n","    test_vocab = {'<s>':2, 'the':6, 'man':2, 'walks': 1, 'dog': 1, 'in': 1, 'park': 1, 'i': 1, 'saw': 1, 'with': 1, 'telescope':1, 'on': 1, CBOW_UNK: 1, '</s>': 2}\n","    print(\"Sample dataset:\")\n","    for x in test_sents: print(x)\n","\n","    context_sizes = [1,3,5]\n","\n","    print('\\n--- TEST: training_examples ---')\n","    training_examples_expected = [[(['<s>', 'man'], 'the'), (['the', 'walks'], 'man'), (['man', 'the'], 'walks'), (['walks', 'dog'], 'the'), (['the', 'in'], 'dog'), (['dog', 'the'], 'in'), (['in', 'park'], 'the'), (['the', '</s>'], 'park'), (['<s>', 'saw'], 'i'), (['i', 'the'], 'saw'), (['saw', 'man'], 'the'), (['the', 'with'], 'man'), (['man', 'the'], 'with'), (['with', 'telescope'], 'the'), (['the', 'on'], 'telescope'), (['telescope', 'the'], 'on'), (['on', '<UNK>'], 'the'), (['the', '</s>'], '<UNK>')],\n","                                  [(['<s>', 'the', 'man', 'the', 'dog', 'in'], 'walks'), (['the', 'man', 'walks', 'dog', 'in', 'the'], 'the'), (['man', 'walks', 'the', 'in', 'the', 'park'], 'dog'), (['walks', 'the', 'dog', 'the', 'park', '</s>'], 'in'), (['<s>', 'i', 'saw', 'man', 'with', 'the'], 'the'), (['i', 'saw', 'the', 'with', 'the', 'telescope'], 'man'), (['saw', 'the', 'man', 'the', 'telescope', 'on'], 'with'), (['the', 'man', 'with', 'telescope', 'on', 'the'], 'the'), (['man', 'with', 'the', 'on', 'the', '<UNK>'], 'telescope'), (['with', 'the', 'telescope', 'the', '<UNK>', '</s>'], 'on')],\n","                                  [(['<s>', 'i', 'saw', 'the', 'man', 'the', 'telescope', 'on', 'the', '<UNK>'], 'with'), (['i', 'saw', 'the', 'man', 'with', 'telescope', 'on', 'the', '<UNK>', '</s>'], 'the')]\n","                                 ]\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        has_passed, message = True, ''\n","        training_examples = test_dataset.training_examples\n","        expected = training_examples_expected[i]\n","        if has_passed and len(training_examples) != len(expected):\n","            has_passed, message = False, 'len(training_examples) is incorrect. Expected: ' + str(len(expected)) + '\\tGot: ' + str(len(training_examples))\n","        if has_passed and set([(type(x), len(x)) for x in training_examples]) != {(tuple, 2)}:\n","            has_passed, message = False, 'Each item of training_examples must be a 2-tuple; at least one of your items is not a 2-tuple.'\n","        if has_passed and set([(type(x[0]), type(x[1])) for x in training_examples]) != {(list, str)}:\n","            has_passed, message = False, 'Each item must contain a list of context words and a target word as a string. At least one of your items does not meet this condition.'\n","        if has_passed and sorted(training_examples, key = lambda x: (' '.join(x[0]), x[1])) != sorted(expected, key = lambda x: (' '.join(x[0]), x[1])):\n","            has_passed, message = False, 'training_examples is incorrect (note that the order of the examples does not matter). Expected: '+str(sorted(expected, key = lambda x: (' '.join(x[0]), x[1]))) + '\\tGot: ' + str(sorted(training_examples, key = lambda x: (' '.join(x[0]), x[1])))\n","\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tcontext_size:', c, '\\t'+status, '\\t'+message)\n","\n","    print('\\n--- TEST: idx2word and word2idx dictionaries ---')\n","    expected_word2idx = {'</s>': 0, '<UNK>': 1, '<s>': 2, 'dog': 3, 'i': 4, 'in': 5, 'man': 6, 'on': 7, 'park': 8, 'saw': 9, 'telescope': 10, 'the': 11, 'walks': 12, 'with': 13}\n","    expected_idx2word = {0: '</s>', 1: '<UNK>', 2: '<s>', 3: 'dog', 4: 'i', 5: 'in', 6: 'man', 7: 'on', 8: 'park', 9: 'saw', 10: 'telescope', 11: 'the', 12: 'walks', 13: 'with'}\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        has_passed, message = True, ''\n","        word2idx = test_dataset.word2idx\n","        idx2word = test_dataset.idx2word\n","\n","        has_passed, message = True, ''\n","        if has_passed and (test_dataset.vocab_size != len(test_dataset.word2idx) or test_dataset.vocab_size != len(test_dataset.idx2word)):\n","            has_passed, message = False, 'dataset.vocab_size (' + str(test_dataset.vocab_size) + ') must be the same length as dataset.word2idx (' + str(len(test_dataset.word2idx)) + ') and dataset.idx2word ('+str(len(test_dataset.idx2word)) +').'\n","        if has_passed and (test_dataset.vocab_size != len(expected_word2idx)):\n","            has_passed, message = False, 'Your vocab size is incorrect. Expected: ' + str(len(expected_word2idx)) + '\\tGot: ' + str(test_dataset.vocab_size)\n","        if has_passed and sorted(list(test_dataset.idx2word.keys())) != list(range(0, test_dataset.vocab_size)):\n","            has_passed, message = False, 'dataset.idx2word must have keys ranging from 0 to dataset.vocab_size-1. Keys in your dataset.idx2word: ' + str(sorted(list(test_dataset.idx2word.keys())))\n","        if has_passed and sorted(list(test_dataset.word2idx.keys())) != sorted(list(expected_word2idx.keys())):\n","            has_passed, message = False, 'Your dataset.word2idx has incorrect keys. Expected: ' + str(sorted(list(expected_word2idx.keys()))) + '\\tGot: ' + str(sorted(list(test_dataset.word2idx.keys())))\n","        if has_passed: # Check that word2idx and idx2word are consistent\n","            widx = sorted(list(test_dataset.word2idx.items()))\n","            idxw = sorted(list([(v,k) for k,v in test_dataset.idx2word.items()]))\n","            if not (len(widx) == len(idxw) and all([widx[q] == idxw[q] for q in range(len(widx))])):\n","                has_passed, message = False, 'Your dataset.word2idx and dataset.idx2word are not consistent. dataset.idx2word: ' + str(test_dataset.idx2word) + '\\tdataset.word2idx: ' + str(test_dataset.word2idx)\n","        if has_passed and word2idx != expected_word2idx:\n","            has_passed, message = False, 'Your dataset.word2idx is incorrect. Expected: ' + str(expected_word2idx) + '\\tGot: ' + str(word2idx)\n","        if has_passed and idx2word != expected_idx2word:\n","            has_passed, message = False, 'Your dataset.word2idx is incorrect. Expected: ' + str(expected_idx2word) + '\\tGot: ' + str(idx2word)\n","\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tcontext_size:', c, '\\t'+status, '\\t'+message)\n","\n","    print('\\n--- TEST: len(dataset) ---')\n","    correct_lens = [18,10,2]\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        has_passed = len(test_dataset) == correct_lens[i]\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        if has_passed: message = ''\n","        else: message = 'len(dataset) is incorrect. Expected: ' + str(correct_lens[i]) + '\\tGot: ' + str(len(test_dataset))\n","        print('\\tcontext_size:', c, '\\t'+status, '\\t'+message)\n","\n","    print('\\n--- TEST: __getitem__(self, idx) ---')\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        for j in range(correct_lens[i]):\n","            returned = test_dataset.__getitem__(j)\n","\n","            has_passed, message = True, ''\n","            if has_passed and len(returned) != 2:\n","                has_passed, message = False, 'dataset.__getitem__(idx) must return 2 items. Got ' + str(len(returned)) +' items instead.'\n","            if has_passed and (type(returned[0]) != torch.Tensor or type(returned[1]) != int):\n","                has_passed, message = False, 'The context vector must be a torch.Tensor and the target index must be an int. Got: (' + str(type(returned[0])) + ', ' + str(type(returned[1])) + ')'\n","            if has_passed and (returned[0].shape != torch.randint(0,100,(2*c,)).shape):\n","                has_passed, message = False, 'Shape of first return is incorrect. Expected: ' + str(torch.randint(0,100,(2*c,)).shape) + '.\\tGot: ' + str(returned[0].shape)\n","\n","            status = 'PASSED' if has_passed else 'FAILED'\n","            print('\\tcontext_size:', c, '\\tidx:',str(j),'\\t' if j<10 else '','\\t',status, '\\t'+message)\n","\n","if __name__ == '__main__':\n","    sanityCheckCbowDataset()"],"metadata":{"id":"qapAnsEMeoAq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##<font color='red'>TODO:</font> Define the CBOW Model [20 points]\n","\n","Here, you will define a simple feed-forward neural network that takes in a context vector and predicts the word that completes the context. We provide you with the `CbowModel` class, and you just need to fill in parts of the `__init__(...)` and `forward(...)` functions. Each of these functions is worth <b>10 points</b>.\n","\n","We have provided you with instructions and hints in the comments. In particular, pay attention to the desired shapes; you may find it helpful to print the shape of the tensors as you code. It may also help to keep the PyTorch documentation open for the modules & functions you are using, since they describe input and output dimensions."],"metadata":{"id":"EZhEl32wk5EM"}},{"cell_type":"code","source":["class CbowModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, context_size):\n","        '''\n","        vocab_size: Size of the vocabulary\n","        embed_size: Size of your embedding vectors\n","        hidden_size: Size of hidden layer of neural network\n","        context_size: The size of your context window used to generate training examples\n","        '''\n","        super(CbowModel, self).__init__()\n","\n","        self.context_size = context_size\n","\n","        ##### TODO #####\n","        # 1. Create an embedding layer using nn.Embedding, that will take an index in your vocabulary as input\n","        #    (referring to a word) and return a vector of size embed_size (i.e. your embedding vector).\n","        #    Note that providing a word index to nn.Embedding is the same (conceptually) as providing a one-hot\n","        #    vector to nn.Linear (however, nn.Embedding takes sparsity into account, so is more efficient)\n","\n","        # 2. Create a linear layer that projects your embedding vector to a vector of size hidden_size.\n","\n","        # 3. Create an output linear layer, that projects your hidden vector to a vector the size of your vocabulary.\n","\n","    def forward(self, inputs):\n","        '''\n","        inputs: Tensor of size [batch_size, 2*context_size]\n","\n","        Returns output: Tensor of size [batch_size, vocab_size]\n","        '''\n","\n","        ##### TODO #####\n","        # 1. Feed the inputs through your embedding layer to get a tensor of size [batch_size, 2*context size, embed_size]\n","        # 2. Average the embedding vectors of each of your context word embeddings (for each example in your batch).\n","        #    Expected size: [batch_size, embed_size]\n","        # 3. Feed this through your linear layer and then a ReLU activation. Expected size: [batch_size, hidden_size]\n","        # 4. Feed this through your output layer and return the result. Expected size [batch_size, vocab_size]\n","        #    Do NOT apply a softmax to the final output - this is done in the training method!\n","\n","        return None"],"metadata":{"id":"osmSxlkFBsx0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sanity Check: CBOW Model"],"metadata":{"id":"sLlw_pe2uNyJ"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","count_parameters = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def makeCbowSanityBatch(test_params):\n","    batch_size = test_params['batch_size']\n","    new_test_params = {k:v for k,v in test_params.items() if k != 'batch_size'}\n","    batch = torch.randint(0, new_test_params['vocab_size'], (batch_size,new_test_params['context_size']*2))\n","    return batch, new_test_params\n","\n","def sanityCheckModel(all_test_params, NN, expected_outputs, init_or_forward, make_batch_fxn=None):\n","    print('--- TEST: ' + ('Number of Model Parameters (tests __init__(...))' if init_or_forward=='init' else 'Output shape of forward(...)') + ' ---')\n","\n","    for tp_idx, (test_params, expected_output) in enumerate(zip(all_test_params, expected_outputs)):\n","        if init_or_forward == \"forward\":\n","            input, test_params = make_batch_fxn(test_params)\n","\n","        # Construct the student model\n","        tps = {k:v for k, v in test_params.items()}\n","        stu_nn = NN(**tps)\n","\n","        if init_or_forward == \"forward\":\n","            with torch.no_grad():\n","                stu_out = stu_nn(input)\n","            ref_out_shape = expected_output\n","\n","            has_passed = torch.is_tensor(stu_out)\n","            if not has_passed: msg = 'Output must be a torch.Tensor; received ' + str(type(stu_out))\n","            else:\n","                has_passed = stu_out.shape == ref_out_shape\n","                msg = 'Your Output Shape: ' + str(stu_out.shape)\n","\n","\n","            status = 'PASSED' if has_passed else 'FAILED'\n","            message = '\\t' + status + \"\\t Init Input: \" + str({k:v for k,v in tps.items()}) + '\\tForward Input Shape: ' + str(input.shape) + '\\tExpected Output Shape: ' + str(ref_out_shape) + '\\t' + msg\n","            print(message)\n","        else:\n","            stu_num_params = count_parameters(stu_nn)\n","            ref_num_params = expected_output\n","            comparison_result = (stu_num_params == ref_num_params)\n","\n","            status = 'PASSED' if comparison_result else 'FAILED'\n","            message = '\\t' + status + \"\\tInput: \" + str({k:v for k,v in test_params.items()}) + ('\\tExpected Num. Params: ' + str(ref_num_params) + '\\tYour Num. Params: '+ str(stu_num_params))\n","            print(message)\n","\n","        del stu_nn\n","\n","\n","if __name__ == '__main__':\n","    # Test init\n","    cbow_init_inputs = [{'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 128, 'context_size': 4}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 128, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 128, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 128, 'context_size': 4}]\n","    cbow_init_expected_outputs = [3082, 3082, 5834, 5834, 5450, 5450, 10250, 10250, 99112, 99112, 165224, 165224, 133160, 133160, 201320, 201320]\n","\n","    sanityCheckModel(cbow_init_inputs, CbowModel, cbow_init_expected_outputs, \"init\")\n","    print()\n","\n","    # Test forward\n","    cbow_forward_inputs = [{'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}]\n","    cbow_forward_expected_outputs = [torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000])]\n","\n","    sanityCheckModel(cbow_forward_inputs, CbowModel, cbow_forward_expected_outputs, \"forward\", makeCbowSanityBatch)"],"metadata":{"id":"x7IJDXGkuQlz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train the CBOW Model [15 points]\n","\n","Now, we initialize the <b>dataloader</b>. A dataloader is responsible for providing batches of data to your model. Notice how we first instantiate dataset.\n","\n","You do not need to edit this cell."],"metadata":{"id":"QekcRZjslC93"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","BATCH_SIZE = 1000 # You may change the batch size if you'd like\n","CONTEXT_SIZE = 3  # You may change the context size if you'd like\n","\n","if __name__=='__main__':\n","    cbow_dataset = CbowDataset(sentences, vocab, CONTEXT_SIZE)\n","    cbow_dataloader = torch.utils.data.DataLoader(cbow_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)"],"metadata":{"id":"q17j2AwNZ20J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we provide you with a function that takes your model and trains it on the data.\n","\n","You do not need to edit this cell. However, you may want to write code to save your model periodically, as Colab connections are not permanent. See the tutorial here if you wish to do this: https://pytorch.org/tutorials/beginner/saving_loading_models.html."],"metadata":{"id":"fVAnU3-L7Md5"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","from tqdm.notebook import tqdm\n","from torch import optim\n","\n","def train_cbow_model(model, num_epochs, data_loader, optimizer, criterion):\n","    print(\"Training CBOW model....\")\n","    for epoch in range(num_epochs):\n","        epoch_loss, n = 0, 0\n","        for context, target in tqdm(data_loader):\n","            optimizer.zero_grad()\n","            log_probs = model(context.long().to(DEVICE)) # to(torch.float32)\n","            loss = criterion(log_probs, target.to(DEVICE))\n","            loss.backward()\n","            optimizer.step()\n","            n += context.shape[0]\n","            epoch_loss += (loss*context.shape[0])\n","\n","        epoch_loss = epoch_loss/n\n","        print('[TRAIN]\\t Epoch: {:2d}\\t Loss: {:.4f}'.format(epoch+1, epoch_loss))\n","    print('CBOW Model Trained!\\n')"],"metadata":{"id":"9Hl52H5T7MGV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sogetmSb8z2I"},"source":["Now you can instantiate your model. We provide you with some recommended hyperparameters; you should be able to get the desired accuracy with these, but feel free to play around with them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHmJio5W8z2I"},"outputs":[],"source":["if __name__=='__main__':\n","    cbow_model = CbowModel(vocab_size = cbow_dataset.vocab_size, # Don't change this\n","                embed_size = 128, # Feel free to change\n","                hidden_size = 128, # Feel free to change\n","                context_size = CONTEXT_SIZE) # Don't change this (though you may change the value of CONTEXT_SIZE above if you wish)\n","\n","    # Put your model on the device (cuda or cpu)\n","    cbow_model = cbow_model.to(DEVICE)\n","\n","    print('The model has {:,d} trainable parameters'.format(count_parameters(cbow_model)))"]},{"cell_type":"markdown","source":["Next, we create the **criterion**, which is our loss function: it is a measure of how well the model matches the empirical distribution of the data. We use cross-entropy loss (https://en.wikipedia.org/wiki/Cross_entropy).\n","\n","We also define the **optimizer**, which performs gradient descent. We use the Adam optimizer (https://arxiv.org/pdf/1412.6980.pdf), which has been shown to work well on these types of models."],"metadata":{"id":"GV2sajpX9mN-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTlIN_Nz9nsx"},"outputs":[],"source":["import torch.optim as optim\n","\n","if __name__=='__main__':\n","    LEARNING_RATE = 0.01 # Feel free to try other learning rates\n","\n","    # Define the loss function\n","    criterion = nn.CrossEntropyLoss().to(DEVICE)\n","\n","    # Define the optimizer\n","    optimizer = optim.Adam(cbow_model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"e-WrKoEW9nsx"},"source":["Finally, we can train the model. If the model is implemented correctly and you're using the GPU, this cell should take around <b>3 minutes</b> (or less). Feel free to change the number of epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7BlulY89nsx"},"outputs":[],"source":["if __name__=='__main__':\n","    N_EPOCHS = 6 # Feel free to change this\n","\n","    # Train model for N_EPOCHS epochs\n","    train_cbow_model(cbow_model, N_EPOCHS, cbow_dataloader, optimizer, criterion)"]},{"cell_type":"markdown","source":["To get full credit on the word embeddings, you must return the correct vectors when your model is instantiated with a particular random seed and called on the autograder. This is worth <b>15 points</b>."],"metadata":{"id":"xLNHHFgoB7Tb"}},{"cell_type":"markdown","source":["## Visualize Word Embeddings\n","\n","Now that you have a trained model, we can extract the word embeddings and visualize them. The word embeddings are basically the weight matrix of the embedding layer that you defined, as this maps each index of your vocab to a dense vector of size `embed_size`.\n","\n","Since we cannot easily visualize such high-dimensional vectors, we use a process called TSNE (t-distributed stochastic neighbor embedding). This reduces the vectors to a 2-dimensional space so that we can visualize them. For more information on TSNE, see https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). Note that this method is not deterministic, so running this cell multiple times will give you a different visualization.\n","\n","The cell below will run TSNE and plot the word embeddings corresponding to thed 1,000 most frequent words on a 2-dimensional plot. You are welcome to increase this threshold if you'd like to see the vectors for more words."],"metadata":{"id":"iWea2Lgh-MBX"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    from sklearn.manifold import TSNE\n","    import numpy as np\n","    import plotly.express as px\n","    import pandas as pd\n","    import warnings\n","    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","    THRESHOLD = 1000\n","    words = [x[0] for x in sorted(vocab.items(), key = lambda x: -x[1])[:THRESHOLD]]\n","    idxes = [cbow_dataset.word2idx[word] for word in words]\n","    vectors = np.array([cbow_model.embedding.weight[i].tolist() for i in idxes])\n","\n","    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, verbose=False)\n","    new_vectors = tsne_model.fit_transform(vectors)\n","\n","    df = pd.DataFrame(data={'x': new_vectors[:,0], 'y': new_vectors[:,1], 'word':words})\n","\n","    fig = px.scatter(df, x='x', y='y', text='word')\n","    fig.update_traces(textposition='top center')\n","    fig.update_layout(height=600, title_text='Word Embedding 2D Visualization')\n","    fig.show()"],"metadata":{"id":"A5e4xjOIS3nS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["At a high level, you should see words with similar meaning clustering together. You can use your mouse to zoom in and inspect the vector space closer.\n","\n","You should also see mini-clusters within this plot; you would need to zoom into examine these. Examples of mini-clusters you might see are:\n","* <b>Time words:</b> hours, minutes, seconds, months, weeks, years, etc.\n","* <b>Years:</b> 2000, 2002, 2004, etc.\n","* <b>Numbers:</b> 10, 15, 37, etc.\n","* <b>Months:</b> january, february, march, etc. <em>Question: does the word 'may', which is both a month and a modal verb, cluster with the other months? If not, can you see where it is in relation to other modal verbs ('can', 'will', 'would', 'might', etc.)?</em>\n","\n","Feel free to increase the number of vectors plotted if you want to investigate further."],"metadata":{"id":"8qNOeI2Wn76l"}},{"cell_type":"markdown","metadata":{"id":"zHbJ1-aDsWCG"},"source":["# Part 2: Train a Convolutional Neural Network (CNN) [50 points]\n","\n","The second part of this homework concerns text classification. You will train a CNN classifier to determine the sentiment of movie reviews."]},{"cell_type":"markdown","metadata":{"id":"bMVBA0ijAUgt"},"source":["## Download & Preprocess the Data\n","We will be using the IMDb movie reviews dataset, which is a corpus of movie reviews along with a <em>positive</em> or <em>negative</em> classification. This is provided by Huggingface using the datasets library."]},{"cell_type":"markdown","source":["The following cell will produce `train_data` and `test_data`. It also does some basic tokenization.\n","\n","*   To access the list of textual tokens for the *i*th example, use `train_data[i][1]`\n","*   To access the label for the *i*th example, use `train_data[i][0]`"],"metadata":{"id":"O_QBJg2d_1qY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfX3bNby8FYL"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import os\n","import random\n","import torch\n","from datasets import load_dataset\n","\n","def cnn_preprocess(review):\n","    '''\n","    Simple preprocessing function.\n","    '''\n","    res = []\n","    for x in review.split(' '):\n","        remove_beg=True if x[0] in {'(', '\"', \"'\"} else False\n","        remove_end=True if x[-1] in {'.', ',', ';', ':', '?', '!', '\"', \"'\", ')'} else False\n","        if remove_beg and remove_end: res += [x[0], x[1:-1], x[-1]]\n","        elif remove_beg: res += [x[0], x[1:]]\n","        elif remove_end: res += [x[:-1], x[-1]]\n","        else: res += [x]\n","    return res\n","\n","if __name__=='__main__':\n","    train_data = load_dataset(\"imdb\")\n","    train_data = train_data['train']\n","    train_data = list(train_data)\n","    train_data = [(x['label'], cnn_preprocess(x['text'])) for x in train_data]\n","    train_data, test_data = train_data[0:10000] + train_data[12500:12500+10000], train_data[10000:12500] + train_data[12500+10000:],\n","\n","    print('Num. Train Examples:', len(train_data))\n","    print('Num. Test Examples:', len(test_data))\n","\n","    # Make pos/neg\n","    train_data = [('neg' if x[0] == 1 else 'pos', x[1]) for x in train_data]\n","    test_data = [('neg' if x[0] == 1 else 'pos', x[1]) for x in test_data]\n","\n","    print(\"\\nSAMPLE DATA:\")\n","    for x in random.sample(train_data, 5):\n","        print('Sample text:', x[1])\n","        print('Sample label:', x[0], '\\n')"]},{"cell_type":"markdown","metadata":{"id":"lvFX-iX5oq7T"},"source":["## <font color='red'>TODO:</font> Define the Dataset Class [10 Points]\n","\n","In the following cell, we will define the <b>dataset</b> class. The dataset contains the tokenized data for your model. You need to implement the following functions:\n","\n","*   <b>` build_dictionary(self)`:</b>  <b>[5 points]</b> Creates the dictionaries `idx2word` and `word2idx`. You will represent each word in the dataset with a unique index, and keep track of this in these dictionaries. Use the hyperparameter `threshold` to control which words appear in the dictionary: a training wordâ€™s frequency should be `>= threshold` to be included in the dictionary.\n","\n","* <b>`convert_text(self)`:</b> Converts each review in the dataset to a list of indices, given by your `word2idx` dictionary. You should store this in the `textual_ids` variable, and the function does not return anything. If a word is not present in the  `word2idx` dictionary, you should use the `<UNK>` token for that word. Be sure to append the `<END>` token to the end of each review.\n","\n","*   <b>` get_text(self, idx) `:</b> Return the review at `idx` in the dataset as an array of indices corresponding to the words in the review. If the length of the review is less than `max_len`, you should pad the review with the `<PAD>` character up to the length of `max_len`. If the length is greater than `max_len`, then it should only return the first `max_len` words. The return type should be `torch.LongTensor`.\n","\n","*   <b>`get_label(self, idx) `</b>: Return the value `1` if the label for `idx` in the dataset is `positive`, and should return `0` if it is `negative`. The return type should be `torch.LongTensor`.\n","\n","*  <b> ` __len__(self) `:</b> Return the total number of reviews in the dataset as an `int`.\n","\n","*   <b>` __getitem__(self, idx)`:</b> <b>[5 points]</b> Return the (padded) text, and the label. The return type for both these items should be `torch.LongTensor`. You should use the ` get_label(self, idx) ` and ` get_text(self, idx) ` functions here.\n","\n","\n","<b>Note:</b> You should convert all words to lower case in your functions.\n","\n","<font color='green'><b>Hint:</b> Make sure that you use instance variables such as `self.threshold` throughout your code, rather than the global variable `THRESHOLD` (defined later on). The variable `THRESHOLD` will not be known to the autograder, and the use of it within the class will cause an autograder error.</font>\n","\n","<font color='green'><b>Hint:</b> Make sure that your dataset is deterministic $-$ that is, if it is instantiated multiple times, then the `word2idx` and `idx2word` mappings are the same. If they are not, the autograder will be unable to evaluate your CNN classifications.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1irMn3LX2YDB"},"outputs":[],"source":["CNN_PAD = '<PAD>'\n","CNN_END = '<END>'\n","CNN_UNK = '<UNK>'\n","\n","from torch.utils import data\n","from collections import defaultdict\n","\n","class TextDataset(data.Dataset):\n","    def __init__(self, examples, split, threshold, max_len, idx2word=None, word2idx=None):\n","        ##### DO NOT EDIT #####\n","\n","        self.examples = examples\n","        assert split in {'train', 'val', 'test'}\n","        self.split = split\n","        self.threshold = threshold\n","        self.max_len = max_len\n","\n","        # Dictionaries\n","        self.word2idx = word2idx # Mapping of word to index\n","        self.idx2word = idx2word # Mapping of index to word\n","        if split == 'train':\n","            self.build_dictionary()\n","        self.vocab_size = len(self.word2idx)\n","\n","        # Convert text to indices\n","        self.textual_ids = []\n","        self.convert_text()\n","\n","\n","    def build_dictionary(self):\n","        '''\n","        Build the dictionaries idx2word and word2idx. This is only called when split='train', as these\n","        dictionaries are passed in to the __init__(...) function otherwise. Be sure to use self.threshold\n","        to control which words are assigned indices in the dictionaries.\n","        Returns nothing.\n","        '''\n","        assert self.split == 'train'\n","\n","        # Don't change this\n","        self.idx2word = {0:CNN_PAD, 1:CNN_END, 2: CNN_UNK}\n","        self.word2idx = {CNN_PAD:0, CNN_END:1, CNN_UNK: 2}\n","\n","        ##### TODO #####\n","        # Count the frequencies of all words in the training data (self.examples)\n","        # Assign idx (starting from 3) to all words having word_freq >= self.threshold\n","        # Make sure you call word.lower() on each word to convert it to lowercase\n","\n","        pass\n","\n","    def convert_text(self):\n","        '''\n","        Convert each review in the dataset (self.examples) to a list of indices, given by self.word2idx.\n","        Store this in self.textual_ids; returns nothing.\n","        '''\n","\n","        ##### TODO #####\n","        # Remember to replace a word with the <UNK> token if it does not exist in the word2idx dictionary.\n","        # Remember to append the <END> token to the end of each review.\n","\n","        pass\n","\n","    def get_text(self, idx):\n","        '''\n","        Return the review at idx as a long tensor (torch.LongTensor) of integers corresponding to the words in the review.\n","        You may need to pad as necessary (see above).\n","        '''\n","\n","        ##### TODO #####\n","\n","        return None\n","\n","    def get_label(self, idx):\n","        '''\n","        This function should return the value 1 if the label for idx in the dataset is 'positive',\n","        and 0 if it is 'negative'. The return type should be torch.LongTensor.\n","        '''\n","\n","        ##### TODO #####\n","\n","        return None\n","\n","    def __len__(self):\n","        '''\n","        Return the number of reviews (int value) in the dataset\n","        '''\n","\n","        ##### TODO #####\n","\n","        return None\n","\n","    def __getitem__(self, idx):\n","        '''\n","        Return the review, and label of the review specified by idx.\n","        '''\n","\n","        ##### TODO #####\n","\n","        return None, None"]},{"cell_type":"markdown","metadata":{"id":"HxVxiGGbFJAj"},"source":["##Sanity Check: Dataset Class\n","\n","The code below runs a sanity check for your `Dataset` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvHIZt8Z-RzK"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def sanityCheckTextDataset():\n","    #\tRead in the sample corpus\n","    reviews = [('pos', 'Your life is good when you have money, success and health'),\n","               ('neg', 'Life is bad when you got not a lot')]\n","    data = [(x[0], cnn_preprocess(x[1])) for x in reviews]\n","    print(\"Sample dataset:\")\n","    for x in data: print(x)\n","\n","    thresholds = [1,2,3]\n","    print('\\n--- TEST: idx2word and word2idx dictionaries ---') # max_len does not matter for this test\n","    correct = [[',', '<END>', '<PAD>', '<UNK>', 'a', 'and', 'bad', 'good', 'got', 'have', 'health', 'is', 'life', 'lot', 'money', 'not', 'success', 'when', 'you', 'your'], ['<END>', '<PAD>', '<UNK>', 'is', 'life', 'when', 'you'], ['<END>', '<PAD>', '<UNK>']]\n","    for i in range(len(thresholds)):\n","        dataset = TextDataset(data, 'train', threshold=thresholds[i], max_len=3)\n","\n","        has_passed, message = True, ''\n","        if has_passed and (dataset.vocab_size != len(dataset.word2idx) or dataset.vocab_size != len(dataset.idx2word)):\n","            has_passed, message = False, 'dataset.vocab_size (' + str(dataset.vocab_size) + ') must be the same length as dataset.word2idx (' + str(len(dataset.word2idx)) + ') and dataset.idx2word ('+str(len(dataset.idx2word)) +').'\n","        if has_passed and (dataset.vocab_size != len(correct[i])):\n","            has_passed, message = False, 'Your vocab size is incorrect. Expected: ' + str(len(correct[i])) + '\\tGot: ' + str(dataset.vocab_size)\n","        if has_passed and sorted(list(dataset.idx2word.keys())) != list(range(0, dataset.vocab_size)):\n","            has_passed, message = False, 'dataset.idx2word must have keys ranging from 0 to dataset.vocab_size-1. Keys in your dataset.idx2word: ' + str(sorted(list(dataset.idx2word.keys())))\n","        if has_passed and sorted(list(dataset.word2idx.keys())) != correct[i]:\n","            has_passed, message = False, 'Your dataset.word2idx has incorrect keys. Expected: ' + str(correct[i]) + '\\tGot: ' + str(sorted(list(dataset.word2idx.keys())))\n","        if has_passed: # Check that word2idx and idx2word are consistent\n","            widx = sorted(list(dataset.word2idx.items()))\n","            idxw = sorted(list([(v,k) for k,v in dataset.idx2word.items()]))\n","            if not (len(widx) == len(idxw) and all([widx[q] == idxw[q] for q in range(len(widx))])):\n","                has_passed, message = False, 'Your dataset.word2idx and dataset.idx2word are not consistent. dataset.idx2word: ' + str(dataset.idx2word) + '\\tdataset.word2idx: ' + str(dataset.word2idx)\n","\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tthreshold:', thresholds[i], '\\tmax_len:', 3, '\\t'+status, '\\t'+message)\n","\n","    print('\\n--- TEST: len(dataset) ---')\n","    has_passed = len(dataset) == 2\n","    if has_passed: print('\\tPASSED')\n","    else: print('\\tlen(dataset) is incorrect. Expected: 2\\tGot: ' + str(len(dataset)))\n","\n","    print('\\n--- TEST: __getitem__(self, idx) ---')\n","    max_lens = [3,8,15]\n","    idxes = [0,1]\n","    combos = [{'threshold': t, 'max_len': m, 'idx': idx} for t in thresholds for m in max_lens for idx in idxes]\n","    correct = [(torch.tensor([3, 4, 5]), torch.tensor(1)), (torch.tensor([ 4,  5, 15]), torch.tensor(0)), (torch.tensor([ 3,  4,  5,  6,  7,  8,  9, 10]), torch.tensor(1)), (torch.tensor([ 4,  5, 15,  7,  8, 16, 17, 18]), torch.tensor(0)), (torch.tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  1,  0,  0]), torch.tensor(1)), (torch.tensor([ 4,  5, 15,  7,  8, 16, 17, 18, 19,  1,  0,  0,  0,  0,  0]), torch.tensor(0)), (torch.tensor([2, 3, 4]), torch.tensor(1)), (torch.tensor([3, 4, 2]), torch.tensor(0)), (torch.tensor([2, 3, 4, 2, 5, 6, 2, 2]), torch.tensor(1)), (torch.tensor([3, 4, 2, 5, 6, 2, 2, 2]), torch.tensor(0)), (torch.tensor([2, 3, 4, 2, 5, 6, 2, 2, 2, 2, 2, 2, 1, 0, 0]), torch.tensor(1)), (torch.tensor([3, 4, 2, 5, 6, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0]), torch.tensor(0)), (torch.tensor([2, 2, 2]), torch.tensor(1)), (torch.tensor([2, 2, 2]), torch.tensor(0)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2]), torch.tensor(1)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2]), torch.tensor(0)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0]), torch.tensor(1)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0]), torch.tensor(0))]\n","    for i in range(len(combos)):\n","        combo = combos[i]\n","        dataset = TextDataset(data, 'train', threshold=combo['threshold'], max_len=combo['max_len'])\n","        returned = dataset.__getitem__(combo['idx'])\n","\n","        has_passed, message = True, ''\n","        if has_passed and len(returned) != 2:\n","            has_passed, message = False, 'dataset.__getitem__(idx) must return 2 items. Got ' + str(len(returned)) +' items instead.'\n","        if has_passed and (type(returned[0]) != torch.Tensor or type(returned[1]) != torch.Tensor):\n","            has_passed, message = False, 'Both returns must be of type torch.Tensor. Got: (' + str(type(returned[0])) + ', ' + str(type(returned[1])) + ')'\n","        if has_passed and (returned[0].shape != correct[i][0].shape):\n","            has_passed, message = False, 'Shape of first return is incorrect. Expected: ' + str(correct[i][0].shape) + '.\\tGot: ' + str(returned[0].shape)\n","        if has_passed and (returned[1].shape != correct[i][1].shape):\n","            has_passed, message = False, 'Shape of second return is incorrect. Expected: ' + str(correct[i][1].shape) + '.\\tGot: ' + str(returned[1].shape) + '\\n\\t\\tHint: torch.Size([]) means that the tensor should be dimensionless (just a number). Try squeezing your result.'\n","        if has_passed and (returned[1] != correct[i][1]):\n","            has_passed, message = False, 'Label (second return) is incorrect. Expected: ' + str(correct[i][1]) + '.\\tGot: ' + str(returned[1])\n","        if has_passed:\n","            correct_padding_idxes, your_padding_idxes = torch.where(correct[i][0] == 0)[0], torch.where(returned[0] == dataset.word2idx[CNN_PAD])[0]\n","            if not (correct_padding_idxes.shape == your_padding_idxes.shape and torch.all(correct_padding_idxes == your_padding_idxes)):\n","                has_passed, message = False, 'Padding is not correct. Expected padding indxes: ' + str(correct_padding_idxes) + '.\\tYour padding indexes: ' + str(your_padding_idxes)\n","\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tthreshold:', combo['threshold'], '\\tmax_len:', combo['max_len'] , '\\tidx:', combo['idx'], '\\t'+status, '\\t'+message)\n","\n","if __name__ == '__main__':\n","    sanityCheckTextDataset()"]},{"cell_type":"markdown","metadata":{"id":"CR4VQbQCNZH6"},"source":["The following cell builds the dataset on the IMDb movie reviews and prints an example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSxpGXj6ml9N"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    train_dataset = TextDataset(train_data, 'train', threshold=10, max_len=150)\n","    print('Vocab size:', train_dataset.vocab_size, '\\n')\n","\n","    randidx = random.randint(0, len(train_dataset)-1)\n","    text, label = train_dataset[randidx]\n","    print('Example text:')\n","    print(train_data[randidx][1])\n","    print(text)\n","    print('\\nExample label:')\n","    print(train_data[randidx][0])\n","    print(label)"]},{"cell_type":"markdown","metadata":{"id":"VcSKydlClwOC"},"source":["## <font color='red'>TODO:</font> Define the CNN Model [20 points]\n","Here you will define your convolutional neural network for text classification. We provide you with the CNN class, you need to fill in parts of the `__init__(...)` and `forward(...)` functions. Each of these functions is worth <b>10 points</b>.\n","\n","We have provided you with instructions and hints in the comments. In particular, pay attention to the desired shapes; you may find it helpful to print the shape of the tensors as you code. It may also help to keep PyTorch documentation open for the modules & functions you are using, since they describe input and output dimensions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ztuy2hUaAof"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embed_size, out_channels, filter_heights, stride, dropout, num_classes, pad_idx):\n","        super(CNN, self).__init__()\n","\n","        ##### TODO #####\n","        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","        #   to represent the words in your vocabulary. Make sure to use vocab_size, embed_size, and pad_idx here.\n","\n","        # Define multiple Convolution layers (nn.Conv1d) with filter (kernel) size [filter_height, embed_size] based on your\n","        #   different filter_heights.\n","        # Input channels will be embed_size and output channels will be out_channels (these many different filters will be trained\n","        #   for each convolution layer)\n","        # If you want, you can store a list of modules inside nn.ModuleList.\n","\n","        # Create a dropout layer (nn.Dropout) using dropout\n","\n","        # Define a linear layer (nn.Linear) that consists of num_classes units\n","        #   and takes as input the concatenated output for all cnn layers (out_channels * num_of_cnn_layers units)\n","\n","        pass\n","\n","\n","    def forward(self, texts):\n","        \"\"\"\n","        texts: LongTensor [batch_size, max_len]\n","\n","        Returns output: Tensor [batch_size, num_classes]\n","        \"\"\"\n","\n","        ##### TODO #####\n","\n","        # Pass texts through your embedding layer to convert from word ids to word embeddings\n","        #   Resulting: shape: [batch_size, max_len, embed_size]\n","\n","        # Pass these texts to each of your conv layers and compute their output as follows:\n","        #   Your cnn output will have shape [batch_size, out_channels, *] where * depends on filter_height and stride\n","        #   Apply non-linearity on it (F.relu() is a commonly used one. Feel free to try others)\n","        #   Take the max value across last dimension to have shape [batch_size, out_channels]\n","        # Concatenate (torch.cat) outputs from all your cnns [batch_size, (out_channels*num_of_cnn_layers)]\n","\n","        # Let's understand what you just did:\n","        #   Since each cnn is of different filter_height, it will look at different number of words at a time\n","        #     So, a filter_height of 3 means your cnn looks at 3 words (3-grams) at a time and tries to extract some information from it\n","        #   Each cnn will learn out_channels number of features from the words it sees at a time\n","        #   Then you applied a non-linearity and took the max value for all channels\n","        #     You are essentially trying to find important n-grams from the entire text\n","        # Everything happens on a batch simultaneously hence you have that additional batch_size as the first dimension\n","\n","        # Apply dropout\n","\n","        # Pass your output through the linear layer and return its output\n","        #   Resulting shape: [batch_size, num_classes]\n","\n","        # NOTE: Do NOT apply a sigmoid or softmax to the final output - this is done in the training method!\n","\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"_mVE_ujfnh0w"},"source":["##Sanity Check: CNN Model\n","\n","The code below runs a sanity check for your `CNN` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yy9oF6qUUHvV"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def makeCnnSanityBatch(test_params):\n","    batch_size = test_params['batch_size']\n","    max_len = test_params['max_len']\n","    new_test_params = {k:v for k,v in test_params.items() if k not in {'batch_size', 'max_len'}}\n","    batch = torch.randint(0, new_test_params['vocab_size'], (batch_size,max_len))\n","    return batch, new_test_params\n","\n","if __name__ == '__main__':\n","    # Test init\n","    cnn_init_inputs = [{'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}]\n","    cnn_init_expected_outputs = [22434, 22531, 22434, 22531, 23874, 23939, 23874, 23939, 41730, 42115, 41730, 42115, 47490, 47747, 47490, 47747, 44578, 44675, 44578, 44675, 47554, 47619, 47554, 47619, 82306, 82691, 82306, 82691, 94210, 94467, 94210, 94467]\n","\n","    sanityCheckModel(cnn_init_inputs, CNN, cnn_init_expected_outputs, \"init\")\n","    print()\n","\n","    # Test forward\n","    cnn_forward_inputs = [{'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}]\n","    cnn_forward_expected_outputs = [torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3])]\n","\n","    sanityCheckModel(cnn_forward_inputs, CNN, cnn_forward_expected_outputs, \"forward\", makeCnnSanityBatch)"]},{"cell_type":"markdown","metadata":{"id":"FupiBIfasCu_"},"source":["## Train CNN Model\n","\n","First, we initialize the train and test <b>dataloaders</b>. A dataloader is responsible for providing batches of data to your model. Notice how we first instantiate datasets for the train and test data, and that we use the training vocabulary for both.\n","\n","You do not need to edit this cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2QYl334n9ON"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    THRESHOLD = 5 # Don't change this\n","    MAX_LEN = 200 # Don't change this\n","    BATCH_SIZE = 32 # Feel free to try other batch sizes\n","\n","    train_dataset = TextDataset(train_data, 'train', THRESHOLD, MAX_LEN)\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","    test_dataset = TextDataset(test_data, 'test', THRESHOLD, MAX_LEN, train_dataset.idx2word, train_dataset.word2idx)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"]},{"cell_type":"markdown","metadata":{"id":"AvsctopWmeoY"},"source":["Now we provide you with a function that takes your model and trains it on the data.\n","\n","You do not need to edit this cell. However, you may want to write code to save your model periodically, as Colab connections are not permanent. See the tutorial here if you wish to do this: https://pytorch.org/tutorials/beginner/saving_loading_models.html."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LD-Jj2rUFOzr"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","from tqdm.notebook import tqdm\n","\n","def train_cnn_model(model, num_epochs, data_loader, optimizer, criterion):\n","    print('Training Model...')\n","    model.train()\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        for texts, labels in tqdm(data_loader):\n","            texts = texts.to(DEVICE) # shape: [batch_size, MAX_LEN]\n","            labels = labels.to(DEVICE) # shape: [batch_size]\n","\n","            optimizer.zero_grad()\n","\n","            output = model(texts)\n","            acc = accuracy(output, labels)\n","\n","            loss = criterion(output, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        print('[TRAIN]\\t Epoch: {:2d}\\t Loss: {:.4f}\\t Train Accuracy: {:.2f}%'.format(epoch+1, epoch_loss/len(data_loader), 100*epoch_acc/len(data_loader)))\n","    print('Model Trained!\\n')"]},{"cell_type":"markdown","metadata":{"id":"FyIZS0WUhFA6"},"source":["Here are some other helper functions we will need."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVP2scuyhG5f"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def accuracy(output, labels):\n","    \"\"\"\n","    Returns accuracy per batch\n","    output: Tensor [batch_size, n_classes]\n","    labels: LongTensor [batch_size]\n","    \"\"\"\n","    preds = output.argmax(dim=1) # find predicted class\n","    correct = (preds == labels).sum().float() # convert into float for division\n","    acc = correct / len(labels)\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"YjvX5c6Isw9e"},"source":["Now you can instantiate your model. We provide you with some recommended hyperparameters; you should be able to get the desired accuracy with these, but feel free to play around with them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5UtdjGDuBty"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    cnn_model = CNN(vocab_size = train_dataset.vocab_size, # Don't change this\n","                embed_size = 128,\n","                out_channels = 64,\n","                filter_heights = [2, 3, 4],\n","                stride = 1,\n","                dropout = 0.5,\n","                num_classes = 2, # Don't change this\n","                pad_idx = train_dataset.word2idx[CNN_PAD]) # Don't change this\n","\n","    # Put your model on the device (cuda or cpu)\n","    cnn_model = cnn_model.to(DEVICE)\n","\n","    print('The model has {:,d} trainable parameters'.format(count_parameters(cnn_model)))"]},{"cell_type":"markdown","metadata":{"id":"SeHpqw6zvkhI"},"source":["Next, we create the **criterion**, which is our loss function: it is a measure of how well the model matches the empirical distribution of the data. We use cross-entropy loss (https://en.wikipedia.org/wiki/Cross_entropy).\n","\n","We also define the **optimizer**, which performs gradient descent. We use the Adam optimizer (https://arxiv.org/pdf/1412.6980.pdf), which has been shown to work well on these types of models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoeyQL4PoNoH"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import torch.optim as optim\n","\n","if __name__=='__main__':\n","    LEARNING_RATE = 5e-4 # Feel free to try other learning rates\n","\n","    # Define the loss function\n","    criterion = nn.CrossEntropyLoss().to(DEVICE)\n","\n","    # Define the optimizer\n","    optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"RopLfAJ9wOHN"},"source":["Finally, we can train the model. If the model is implemented correctly and you're using the GPU, this cell should take around <b>4 minutes</b> (or less). Feel free to change the number of epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPOs1FifoNoN"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    N_EPOCHS = 10 # Feel free to change this\n","\n","    # train model for N_EPOCHS epochs\n","    train_cnn_model(cnn_model, N_EPOCHS, train_loader, optimizer, criterion)"]},{"cell_type":"markdown","metadata":{"id":"Q-OJbZ72t6Yq"},"source":["## Evaluate CNN Model [20 points]\n","\n","Now that we have trained a model for text classification, it is time to evaluate it. We have provided you with a function to do this; you do not need to modify anything.\n","\n","To pass the autograder for the CNN, you will need to achieve **82% accuracy** on the hidden test set on Gradescope. Note that the Gradescope test set is very similar, and the accuracies between the two datasets should be comparable.\n","\n","<font color='green'><b>Hint:</b> If you receive close to 82% accuracy in the notebook but close to 50% accuracy in the autograder, then the most likely causes are:\n","1. You uploaded an untrained model checkpoint. Make sure you save the model after it is trained.\n","2. Your `TextDataset` class is not deterministic in that the `word2idx` and `idx2word` mappings are not necessarily in the same order when the class is instantiated multiple times. This is a problem as your trained CNN will expect the words in the order seen in this notebook, but the autograder will be using a different ordering. If this is your issue, reimplement the `TextDataset` class so that it is deterministic, and then retrain and upload your model.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTiiYDZIF--7"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import random\n","\n","def evaluate(model, data_loader, criterion, use_tqdm=False):\n","    print('Evaluating performance on the test dataset...')\n","    has_printed=False\n","    model.eval()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    all_predictions = []\n","    iterator = tqdm(data_loader) if use_tqdm else data_loader\n","    total = 0\n","    for texts, labels in iterator:\n","        bs = texts.shape[0]\n","        total += bs\n","        texts = texts.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        output = model(texts)\n","        acc = accuracy(output, labels) * len(labels)\n","        pred = output.argmax(dim=1)\n","        all_predictions.append(pred)\n","\n","        loss = criterion(output, labels) * len(labels)\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        if random.random() < 0.0015 and bs == 1:\n","            if not has_printed: print(\"\\nSOME PREDICTIONS FROM THE MODEL:\")\n","            print(\"Input: \"+' '.join([data_loader.dataset.idx2word[idx] for idx in texts[0].tolist() if idx not in {data_loader.dataset.word2idx[CNN_PAD], data_loader.dataset.word2idx[CNN_END]}]))\n","            print(\"Prediction:\", pred.item(), '\\tCorrect Output:', labels.item(), '\\n')\n","            has_printed=True\n","\n","    full_acc = 100*epoch_acc/total\n","    full_loss = epoch_loss/total\n","    print('[TEST]\\t Loss: {:.4f}\\t Accuracy: {:.2f}%'.format(full_loss, full_acc))\n","    predictions = torch.cat(all_predictions)\n","    return predictions, full_acc, full_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z718w8e0oNoS"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    evaluate(cnn_model, test_loader, criterion, use_tqdm=True) # Compute test data accuracy"]},{"cell_type":"markdown","metadata":{"id":"8WQAV6O2xHvS"},"source":["# What to Submit\n","\n","To submit the assignment, download this notebook as a <TT>.py</TT> file. You can do this by going to <TT>File > Download > Download .py</TT>. Then (optionally) rename it to `hwk2.py`.\n","\n","You will also need to save the `cnn_model` (you do not need to save anything additional for your word embeddings). You can run the cell below to do this. After you save the files to your Google Drive, you need to manually download the files to your computer, and then submit them to the autograder.\n","\n","You will submit the following files to the autograder:\n","1.   `hwk2.py`, the download of this notebook as a `.py` file (**not** a `.ipynb` file)\n","1.   `cnn.pt`, the saved version of your `cnn_model`\n","\n","**Reminder: Make sure that you access the Gradescope submission page via the corresponding assignment in Coursera!** Failure to do so may result in the inability to push your grades to Coursera. (The same goes for quizzes!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abbbMNi8X_ai"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print()\n","\n","    try:\n","        cnn_model is None\n","        cnn_exists = True\n","    except:\n","        cnn_exists = False\n","\n","    if cnn_exists:\n","        print(\"Saving CNN model....\")\n","        torch.save(cnn_model, \"drive/My Drive/cnn.pt\")\n","\n","    print(\"\\nDone!\")"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}